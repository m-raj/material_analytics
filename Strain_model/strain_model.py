# -*- coding: utf-8 -*-
"""Strain-model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dxverRW1i7LGx-UrNpBsCXi2PWyE0j_i

Downloading dataset
"""

!mv kaggle.json /root/.kaggle/.
!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download gomayank/rve-elasticity-simulation --unzip

import numpy as np
from sklearn.model_selection import train_test_split
import timeit
import tensorflow as tf
from tensorflow.keras import Model, layers, optimizers, losses
from tensorflow import keras
from tensorflow.keras.utils import Sequence, plot_model
from tensorflow.keras.models import load_model
import math

NUM_CLASSES = 6
NUM_ELEMENTS = 31*31*31
VOLUME_FRACS = [10, 30]
NUM_RVE = 100
BATCH_SIZE = 20000
EPOCHS = 1
NUM_WORKERS = 10
NEIGHBORS = 3
LOCAL_LEN = 2*NEIGHBORS + 1
DATASET_PATH = '/content/'

rve_index = np.arange(NUM_RVE)
train_rves, test_rves = train_test_split(rve_index)

class RVE_Dataset(Sequence):
    def __init__(self,
                 dataset_path=DATASET_PATH,
                 num_classes=NUM_CLASSES,
                 vol_fracs=VOLUME_FRACS,
                 ):
        self.dataset_path = dataset_path
        self.num_classes = num_classes
        self.vol_fracs = vol_fracs

        self.current_class = None
        self.current_volume_fraction = None

    def __len__(self):
        return self.num_classes*len(self.vol_fracs)

    def __getitem__(self, index):
        self._set_current_variables(index)
        x = self._read_phase_distribution()
        y = self._get_target_variable()
        return x, y

    def _get_target_variable(self):
        path = self.dataset_path + 'vol_frac_'+str(self.current_volume_fraction) +\
                '/E33/E33_class_' + str(self.current_class)+'.npy'
        return np.load(path)

    def _set_current_variables(self, index):
        self.current_class = (index % self.num_classes) + 1
        index = index//self.num_classes
        self.current_volume_fraction = self.vol_fracs[index % len(self.vol_fracs)]

    def _read_phase_distribution(self):
        path = self.dataset_path + 'vol_frac_'+str(self.current_volume_fraction) +\
                '/phase_distribution/class_' + \
                str(self.current_class)+'.npy'
        return np.load(path)


class IterateOverElements(Sequence):
    def __init__(self,
                 phase_dist,
                 target_value,
                 neighbors,
                 rve_list,
                 batch_size=32):
        self.phase_dist = phase_dist
        self.target_value = target_value
        self.neighbors = neighbors
        self.num_elements = self.phase_dist.shape[0]
        self.rve_list = rve_list
        self.batch_size = batch_size

    def __len__(self):
        return math.ceil((self.num_elements * len(self.rve_list))/self.batch_size)

    def __getitem__(self, index):
        indeces = np.arange(index * self.batch_size, (index+1) * self.batch_size)        
        x, y = [], []
        for i in indeces:
            single_x, single_y = self._get_single_item(i)
            x.append(single_x)
            y.append(single_y)
        return np.asarray(x), np.asarray(y)
        
    
    def _get_single_item(self, index):
        element_id = index % self.num_elements
        rve_id = (index // self.num_elements) % len(self.rve_list)
        x = self._get_neighbors(element_id, self.rve_list[rve_id])
        y = self.target_value[element_id, self.rve_list[rve_id]]
        return x, y
        

    def _get_element_coords(self, element_id):
        coords = np.zeros(3, np.int32)
        for i in range(coords.shape[0]):
            coords[i] = element_id % 31
            element_id = element_id // 31
        return coords

    def _get_neighbors(self, element_id, rve_id):
        element_coords = self._get_element_coords(element_id)
        lower_limits = element_coords - self.neighbors
        L = 2*self.neighbors + 1
        local_phase_distribution = np.ones((L, L, L))*30/100
        for i in range(L):
            x = lower_limits[0]+i
            if x < 0 or x > 30:
                continue
            for j in range(L):
                y = lower_limits[1]+j
                if y < 0 or y > 30:
                    continue
                for k in range(L):
                    z = lower_limits[2]+k
                    if z < 0 or z > 30:
                        continue
                    local_phase_distribution[i, j, k] = self.phase_dist[31*31*z + 31*y + x, rve_id]
            return local_phase_distribution

class Sirius(Model):
    def train_step(self, inputs):
        x, y = inputs
        with tf.GradientTape() as tape:
            y_pred = self.call(x)
            loss = self.compiled_loss(y, y_pred)
        # Compute gradients
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        # Update metrics (includes the metric that tracks the loss)
        self.compiled_metrics.update_state(y, y_pred)
        # Return a dict mapping metric names to current value
        return {m.name: m.result() for m in self.metrics}

fixed_input = layers.Input(shape=(LOCAL_LEN, LOCAL_LEN, LOCAL_LEN), name='fixed_input')
x = layers.Conv2D(120, (3, 3),
                  activation=keras.activations.relu,
                  name='conv1')(fixed_input)
x = layers.Conv2D(120, (3, 3),
                  activation=keras.activations.relu,
                  name='conv2')(x)
x = layers.Flatten(name='flatten')(x)
# x = layers.Dense(50, name='dense1', activation=keras.activations.tanh)(x)
output = layers.Dense(1, name='output')(x)
model = Sirius(inputs=fixed_input, outputs=output)

model.compile(loss=losses.MeanSquaredError(),
              optimizer=optimizers.Adam(learning_rate=1E-3))

callbacks = [keras.callbacks.ReduceLROnPlateau(moniter="val_loss",
                                               factor=0.8,
                                               patience=5,
                                               min_delta=0.001,
                                               cooldown=10,
                                               verbose=1,
                                               min_lr=5E-5)]

with tf.device('/cpu:0'):
  superset = RVE_Dataset()
  for idx, (x, y) in enumerate(superset):
      train_subset = IterateOverElements(x, y, NEIGHBORS, train_rves, BATCH_SIZE)
      test_subset = IterateOverElements(x, y, NEIGHBORS, test_rves, BATCH_SIZE)
      logs = model.fit(train_subset,
                  validation_data=test_subset,
                  callbacks=callbacks,
                  epochs=EPOCHS)
      for key in logs.history.keys():
          np.save(str(idx)+'_'+key, logs.history[key])
      model.save('model')
      break

with tf.device('/device:GPU:0'):
  superset = RVE_Dataset()
  for idx, (x, y) in enumerate(superset):
      train_subset = IterateOverElements(x, y, NEIGHBORS, train_rves, BATCH_SIZE)
      test_subset = IterateOverElements(x, y, NEIGHBORS, test_rves, BATCH_SIZE)
      logs = model.fit(train_subset,
                  validation_data=test_subset,
                  callbacks=callbacks,
                  epochs=EPOCHS)
      for key in logs.history.keys():
          np.save(str(idx)+'_'+key, logs.history[key])
      model.save('model')
      break

class Temp_data(Sequence):
    def __init__(self, input_data, batch_size=1):
      self.input_data = input_data
      self.batch_size = batch_size

    def __len__(self):
      return math.ceil(len(self.input_data)/self.batch_size)

    def __getitem__(self, index):
      indeces = np.arange(index*self.batch_size, (index+1)*self.batch_size)
      data = []
      for temp_index in indeces:
        data.append(self.getitem(temp_index))
      return np.asarray(data), np.asarray(data)

    def getitem(self, index):
      output = self.input_data[index,:,:,:]
      try:
        outuput = self.input_data[i-3:i+3,j-3:j+3,k-3:k+3]
      except:
        output = np.random.uniform(size=(7, 7, 7))
      else:
        output = np.random.uniform(size =(7, 7, 7))
      return output
input_data = np.random.uniform(size=(1,31,31,31))
input_placeholder = layers.Input((31, 31, 31))
x = layers.Conv2D(240, (3, 3))(input_placeholder)
output = layers.Conv2D(240,(3, 3))(x)
model = keras.Model(inputs=input_placeholder, outputs=output)
def new_model():
  _ = model(input_data)